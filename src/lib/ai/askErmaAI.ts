import { askGeminiFromText } from "./gemini";
import { askGPTFromText as askOpenAIFromText, getSystemPrompt } from "./openai";
import type { ChatCompletionMessageParam } from "openai/resources/chat/completions";

export type AIModel = "gemini" | "gpt";

function detectIfVisual(input: string, files?: File[]): AIModel {
  if (files && files.length > 0) return "gemini";
  if (/[ğŸ–¼ï¸ğŸ“·ğŸ“„]/.test(input)) return "gemini";
  return "gpt";
}

function detectLanguageInstruction(question: string): string {
  const q = question.toLowerCase();
  if (/[Ğ°-ÑÑ‘]/i.test(q)) return "bg";
  if (/[Î±-Ï‰Î¬Î­Î®Î¯ÏŒÏÏ]/i.test(q)) return "el";
  if (/[ä½ å¥½|æ—©ä¸Šå¥½]/.test(q)) return "zh";
  if (/[a-z]/i.test(q)) return "en";
  return "";
}

export async function askErmaAI(
  prompt: string,
  contextTextOrHistory: string | ChatCompletionMessageParam[] = [],
  model?: AIModel,
  files?: File[],
): Promise<string> {
  const selectedModel = model || detectIfVisual(prompt, files);
  console.log("ğŸ¤– Ğ˜Ğ·Ğ±Ñ€Ğ°Ğ½ AI Ğ¼Ğ¾Ğ´ĞµĞ»:", selectedModel);
  console.log("ğŸ“¥ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ğ´ĞµĞ½Ğ° ĞºÑŠĞ¼ askErmaAI:", contextTextOrHistory);

  const lang = detectLanguageInstruction(prompt);
  const langPrefix =
    lang === "bg"
      ? "ĞÑ‚Ğ³Ğ¾Ğ²Ğ°Ñ€ÑĞ¹ Ğ½Ğ° Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸, ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ½Ğ¾."
      : lang === "el"
        ? "Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, ÏƒÏÎ½Ï„Î¿Î¼Î± ÎºÎ±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ¬."
        : lang === "zh"
          ? "è¯·ç”¨ä¸­æ–‡ç®€æ´è€Œä¸“ä¸šåœ°å›ç­”ã€‚"
          : lang === "en"
            ? "Respond in English, brief and professional."
            : typeof contextTextOrHistory === "string"
              ? contextTextOrHistory
              : "";

  const systemMessage: ChatCompletionMessageParam = {
    role: "system",
    content: getSystemPrompt().content + "\n\n" + langPrefix,
  };

  if (Array.isArray(contextTextOrHistory)) {
    const limitedHistory = contextTextOrHistory.slice(-10);
    const messages: ChatCompletionMessageParam[] = [
      systemMessage,
      ...limitedHistory,
      { role: "user", content: prompt },
    ];

    if (selectedModel === "gemini") {
      return await askGeminiFromText(messages);
    }

    return await askOpenAIFromText(messages);
  } else {
    const messages: ChatCompletionMessageParam[] = [
      systemMessage,
      { role: "user", content: prompt },
    ];

    if (selectedModel === "gemini") {
      return await askGeminiFromText(messages);
    }

    return await askOpenAIFromText(messages);
  }
}
