import { askGeminiFromText } from "./gemini";
import { askGPTFromText as askOpenAIFromText } from "./openai";
import { getRelevantKnowledge } from "./getRelevantKnowledge";
import type { ChatCompletionMessageParam } from "openai/resources/chat/completions";

export type AIModel = "gemini" | "gpt";

function detectIfVisual(input: string, files?: File[]): AIModel {
  if (files && files.length > 0) return "gemini";
  if (/[ğŸ–¼ï¸ğŸ“·ğŸ“„]/.test(input)) return "gemini";
  return "gpt";
}

function detectLanguageInstruction(question: string): string {
  const q = question.toLowerCase();
  if (/[Ğ°-ÑÑ‘]/i.test(q)) return "bg";
  if (/[Î±-Ï‰Î¬Î­Î®Î¯ÏŒÏÏ]/i.test(q)) return "el";
  if (/[ä½ å¥½|æ—©ä¸Šå¥½]/.test(q)) return "zh";
  if (/[a-z]/i.test(q)) return "en";
  return "";
}

export async function askErmaAI(
  prompt: string,
  contextTextOrHistory: string | ChatCompletionMessageParam[] = [],
  model?: AIModel,
  files?: File[],
): Promise<string> {
  const selectedModel = model || detectIfVisual(prompt, files);
  console.log("ğŸ¤– Ğ˜Ğ·Ğ±Ñ€Ğ°Ğ½ AI Ğ¼Ğ¾Ğ´ĞµĞ»:", selectedModel);

  const lang = detectLanguageInstruction(prompt);

  const langPrefix =
    lang === "bg"
      ? "ĞÑ‚Ğ³Ğ¾Ğ²Ğ°Ñ€ÑĞ¹ Ğ½Ğ° Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸, ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ½Ğ¾."
      : lang === "el"
        ? "Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, ÏƒÏÎ½Ï„Î¿Î¼Î± ÎºÎ±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ¬."
        : lang === "zh"
          ? "è¯·ç”¨ä¸­æ–‡ç®€æ´è€Œä¸“ä¸šåœ°å›ç­”ã€‚"
          : lang === "en"
            ? "Respond in English, brief and professional."
            : "";

  const contextParts = getRelevantKnowledge(prompt);
  console.log("ğŸ“¦ Ğ’Ğ¼ÑŠĞºĞ½Ğ°Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:", contextParts);
  console.log(
    "ğŸ§  Ğ”ÑŠĞ»Ğ¶Ğ¸Ğ½Ğ° Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸):",
    contextParts.join("\n\n").length,
  );

  const systemPrompt: ChatCompletionMessageParam = {
    role: "system",
    content: contextParts.join("\n\n") + "\n\n" + langPrefix,
  };

  if (Array.isArray(contextTextOrHistory)) {
    const limitedHistory = contextTextOrHistory.slice(-10);
    const messages: ChatCompletionMessageParam[] = [
      systemPrompt,
      ...limitedHistory,
      { role: "user", content: prompt },
    ];

    return selectedModel === "gemini"
      ? await askGeminiFromText(messages)
      : await askOpenAIFromText(messages);
  } else {
    const messages: ChatCompletionMessageParam[] = [
      systemPrompt,
      { role: "user", content: prompt },
    ];

    return selectedModel === "gemini"
      ? await askGeminiFromText(messages)
      : await askOpenAIFromText(messages);
  }
}
